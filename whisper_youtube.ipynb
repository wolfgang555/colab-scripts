{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Youtube Videos Transcription with OpenAI's Whisper**\n",
        "\n",
        "[![blog post shield](https://img.shields.io/static/v1?label=&message=Blog%20post&color=blue&style=for-the-badge&logo=openai&link=https://openai.com/blog/whisper)](https://openai.com/blog/whisper)\n",
        "[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)](https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)\n",
        "[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/openai/whisper)](https://github.com/openai/whisper)\n",
        "[![paper shield](https://img.shields.io/static/v1?label=&message=Paper&color=blue&style=for-the-badge&link=https://cdn.openai.com/papers/whisper.pdf)](https://cdn.openai.com/papers/whisper.pdf)\n",
        "[![model card shield](https://img.shields.io/static/v1?label=&message=Model%20card&color=blue&style=for-the-badge&link=https://github.com/openai/whisper/blob/main/model-card.md)](https://github.com/openai/whisper/blob/main/model-card.md)\n",
        "\n",
        "Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\n",
        "\n",
        "This Notebook will guide you through the transcription of a Youtube video using Whisper. You'll be able to explore most inference parameters or use the Notebook as-is to store the transcript and video audio in your Google Drive."
      ],
      "metadata": {
        "id": "96kvih9mXkNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Check GPU type** 🕵️\n",
        "\n",
        "#@markdown The type of GPU you get assigned in your Colab session defined the speed at which the video will be transcribed.\n",
        "#@markdown The higher the number of floating point operations per second (FLOPS), the faster the transcription.\n",
        "#@markdown But even the least powerful GPU available in Colab is able to run any Whisper model.\n",
        "#@markdown Make sure you've selected `GPU` as hardware accelerator for the Notebook (Runtime &rarr; Change runtime type &rarr; Hardware accelerator).\n",
        "\n",
        "#@markdown |  GPU   |  GPU RAM   | FP32 teraFLOPS |     Availability   |\n",
        "#@markdown |:------:|:----------:|:--------------:|:------------------:|\n",
        "#@markdown |  T4    |    16 GB   |       8.1      |         Free       |\n",
        "#@markdown | P100   |    16 GB   |      10.6      |      Colab Pro     |\n",
        "#@markdown | V100   |    16 GB   |      15.7      |  Colab Pro (Rare)  |\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Factory reset your Notebook's runtime if you want to get assigned a new GPU.**\n",
        "\n",
        "!nvidia-smi -L\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QshUbLqpX7L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IfG0E_WbRFI0",
        "cellView": "form",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f669a4b-9576-4378-cf97-f2c3b857f13d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-rise6_yw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-rise6_yw\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.6.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803707 sha256=002f2137674272b45de8946d55d42f0067b2a031d5cc0fbf9eb37b94bc6825af\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-thj7ib49/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.3.31-py3-none-any.whl.metadata (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.2/172.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.3.31-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.3.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Install libraries** 🏗️\n",
        "#@markdown This cell will take a little while to download several libraries, including Whisper.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install yt-dlp\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "import whisper\n",
        "from pathlib import Path\n",
        "import yt_dlp\n",
        "import subprocess\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown, YouTubeVideo\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zwGAsr4sIgd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Optional:** Save data in Google Drive 💾\n",
        "#@markdown Enter a Google Drive path and run this cell if you want to store the results inside Google Drive.\n",
        "\n",
        "# Uncomment to copy generated images to drive, faster than downloading directly from colab in my experience.\n",
        "from google.colab import drive\n",
        "drive_mount_path = Path(\"/\") / \"content\" / \"drive\"\n",
        "drive.mount(str(drive_mount_path))\n",
        "drive_mount_path /= \"My Drive\"\n",
        "#@markdown ---\n",
        "drive_path = \"Colab Notebooks/Whisper Youtube\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change your Google Drive path.**\n",
        "\n",
        "drive_whisper_path = drive_mount_path / Path(drive_path.lstrip(\"/\"))\n",
        "drive_whisper_path.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Model selection** 🧠\n",
        "\n",
        "#@markdown As of the first public release, there are 4 pre-trained options to play with:\n",
        "\n",
        "#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
        "#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
        "#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
        "#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
        "#@markdown | large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
        "\n",
        "#@markdown ---\n",
        "Model = 'medium' #@param ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large']\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the model.**\n",
        "\n",
        "whisper_model = whisper.load_model(Model)\n",
        "\n",
        "if Model in whisper.available_models():\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is selected.**\"\n",
        "    ))\n",
        "else:\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is no longer available.**<br /> Please select one of the following:<br /> - {'<br /> - '.join(whisper.available_models())}\"\n",
        "    ))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TMhrSq_GZ6kA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "afba01c8-9c78-4a2f-dc5b-7be4aafc4fe2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:27<00:00, 55.7MiB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**medium model is selected.**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Video selection** 📺\n",
        "\n",
        "#@markdown Enter the URL of the Youtube video you want to transcribe, wether you want to save the audio file in your Google Drive, and run the cell.\n",
        "\n",
        "Type = \"Youtube video or playlist\" #@param ['Youtube video or playlist', 'Google Drive']\n",
        "#@markdown ---\n",
        "#@markdown #### **Youtube video or playlist**\n",
        "URL = \"https://youtu.be/bAp6qd4Iu0Q?si=JWVB3o8aYx2Qwn-p\" #@param {type:\"string\"}\n",
        "# store_audio = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown #### **Google Drive video, audio (mp4, wav), or folder containing video and/or audio files**\n",
        "video_path = \"Colab Notebooks/transcription/my_video.mp4\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the video.**\n",
        "\n",
        "video_path_local_list = []\n",
        "\n",
        "if Type == \"Youtube video or playlist\":\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': 'm4a/bestaudio/best',\n",
        "        'outtmpl': '%(id)s.%(ext)s',\n",
        "        # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "        'postprocessors': [{  # Extract audio using ffmpeg\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'wav',\n",
        "        }]\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        error_code = ydl.download([URL])\n",
        "        list_video_info = [ydl.extract_info(URL, download=False)]\n",
        "\n",
        "    for video_info in list_video_info:\n",
        "        video_path_local_list.append(Path(f\"{video_info['id']}.wav\"))\n",
        "\n",
        "elif Type == \"Google Drive\":\n",
        "    # video_path_drive = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "    video_path = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "    if video_path.is_dir():\n",
        "        for video_path_drive in video_path.glob(\"**/*\"):\n",
        "            if video_path_drive.is_file():\n",
        "                display(Markdown(f\"**{str(video_path_drive)} selected for transcription.**\"))\n",
        "            elif video_path_drive.is_dir():\n",
        "                display(Markdown(f\"**Subfolders not supported.**\"))\n",
        "            else:\n",
        "                display(Markdown(f\"**{str(video_path_drive)} does not exist, skipping.**\"))\n",
        "            video_path_local = Path(\".\").resolve() / (video_path_drive.name)\n",
        "            shutil.copy(video_path_drive, video_path_local)\n",
        "            video_path_local_list.append(video_path_local)\n",
        "    elif video_path.is_file():\n",
        "        video_path_local = Path(\".\").resolve() / (video_path.name)\n",
        "        shutil.copy(video_path, video_path_local)\n",
        "        video_path_local_list.append(video_path_local)\n",
        "        display(Markdown(f\"**{str(video_path)} selected for transcription.**\"))\n",
        "    else:\n",
        "        display(Markdown(f\"**{str(video_path)} does not exist.**\"))\n",
        "\n",
        "else:\n",
        "    raise(TypeError(\"Please select supported input type.\"))\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    if video_path_local.suffix == \".mp4\":\n",
        "        video_path_local = video_path_local.with_suffix(\".wav\")\n",
        "        result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n"
      ],
      "metadata": {
        "id": "xYLPZQX9S7tU",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3acd359e-af62-4806-b169-74dd39a8d79b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://youtu.be/bAp6qd4Iu0Q?si=JWVB3o8aYx2Qwn-p\n",
            "[youtube] bAp6qd4Iu0Q: Downloading webpage\n",
            "[youtube] bAp6qd4Iu0Q: Downloading tv client config\n",
            "[youtube] bAp6qd4Iu0Q: Downloading player 73381ccc-main\n",
            "[youtube] bAp6qd4Iu0Q: Downloading tv player API JSON\n",
            "[youtube] bAp6qd4Iu0Q: Downloading ios player API JSON\n",
            "[youtube] bAp6qd4Iu0Q: Downloading m3u8 information\n",
            "[info] bAp6qd4Iu0Q: Downloading 1 format(s): 140\n",
            "[download] Destination: bAp6qd4Iu0Q.m4a\n",
            "[download] 100% of   18.45MiB in 00:00:03 at 4.67MiB/s   \n",
            "[FixupM4a] Correcting container of \"bAp6qd4Iu0Q.m4a\"\n",
            "[ExtractAudio] Destination: bAp6qd4Iu0Q.wav\n",
            "Deleting original file bAp6qd4Iu0Q.m4a (pass -k to keep)\n",
            "[youtube] Extracting URL: https://youtu.be/bAp6qd4Iu0Q?si=JWVB3o8aYx2Qwn-p\n",
            "[youtube] bAp6qd4Iu0Q: Downloading webpage\n",
            "[youtube] bAp6qd4Iu0Q: Downloading tv client config\n",
            "[youtube] bAp6qd4Iu0Q: Downloading tv player API JSON\n",
            "[youtube] bAp6qd4Iu0Q: Downloading ios player API JSON\n",
            "[youtube] bAp6qd4Iu0Q: Downloading m3u8 information\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-X0qB9JAzMLY",
        "cellView": "form",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8c4b2d0-5c83-4f26-c8db-1581a5836779"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### bAp6qd4Iu0Q.wav"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:03.000] 大家好,欢迎来到听书学时。\n",
            "[00:03.000 --> 00:11.000] 今天我要给大家介绍由荒木俊哉先生所著的《将脑中的想法进行语言化》这本书。\n",
            "[00:11.000 --> 00:14.000] 今天视频的内容包括以下两个部分。\n",
            "[00:14.000 --> 00:20.000] 第一,如何将脑中的想法语言化,以及如何倾听自己的内心。\n",
            "[00:20.000 --> 00:24.000] 第二,每天只需三分钟,五天内即可见效。\n",
            "[00:24.000 --> 00:28.000] 让任何人都能流畅表达的神奇语言化笔记法。\n",
            "[00:28.000 --> 00:37.000] 首先是第一部分,如何将脑中的想法语言化,以及如何倾听自己的内心这一重要主题开始解说。\n",
            "[00:37.000 --> 00:40.000] 掌握语言化能力可以改变人生。\n",
            "[00:40.000 --> 00:43.000] 你是否曾经遇到过这样的困扰?\n",
            "[00:43.000 --> 00:49.000] 在面试或会议上,突然被要求发言,结果大脑一片空白。\n",
            "[00:49.000 --> 00:55.000] 想要向重要的伴侣或朋友表达自己的想法,却找不到合适的语言。\n",
            "[00:55.000 --> 00:57.000] 事后感到后悔。\n",
            "[00:57.000 --> 01:03.000] 在这些情况下,如果能更自如地表达自己的想法,那该多好。\n",
            "[01:03.000 --> 01:08.000] 如果能更准确地传达自己的情感,那该多棒。\n",
            "[01:08.000 --> 01:11.000] 相信有许多人都曾有过这样的遗憾。\n",
            "[01:11.000 --> 01:16.000] 而本书正是为那些觉得自己不擅长语言表达的人而写的。\n",
            "[01:16.000 --> 01:21.000] 作者指出,语言化能力是任何人都可以习得的技能。\n",
            "[01:22.000 --> 01:29.000] 他与天赋无关,只要掌握正确的方法,并养成一点点习惯,就能得到提升。\n",
            "[01:29.000 --> 01:38.000] 你不需要是学霸,也不需要天生聪明,只要方法正确,任何人都能提升自己的语言表达能力。\n",
            "[01:38.000 --> 01:45.000] 事实上,作者本人作为一名成功的文案撰写人,其初也并不擅长语言表达。\n",
            "[01:45.000 --> 01:49.000] 但经过训练,他逐渐掌握了这项技能。\n",
            "[01:49.000 --> 02:00.000] 曾经的他,在会议上发言时会紧张的脸红,而看到其他文案人员能自如运用语言,他更是感到忘尘莫及。\n",
            "[02:00.000 --> 02:08.000] 然而,正是通过一套简单的方法,他成功克服了这一点,并掌握了高效的表达技巧。\n",
            "[02:08.000 --> 02:18.000] 在不断摸索的过程中,作者创造了语言化笔记法这一方法,并让一千多人进行了实践测试,同时不断改进。\n",
            "[02:18.000 --> 02:24.000] 这一方法成为了他的日常习惯,并彻底改变了他的人生。\n",
            "[02:24.000 --> 02:29.000] 通过使用语言化笔记法,作者的写作速度大幅提升。\n",
            "[02:29.000 --> 02:38.000] 不仅如此,在日常生活中,他还学会了用自己的语言准确表达自己喜欢的电影和电视剧的魅力。\n",
            "[02:38.000 --> 02:46.000] 大家可以想象一下,如果能够迅速清晰表达脑海中的想法,会是一种怎样的体验?\n",
            "[02:46.000 --> 02:52.000] 如果能够准确地向他人传达自己的思考和价值观,人生会变得多么轻松?\n",
            "[02:52.000 --> 02:58.000] 如果能够顺畅地与他人交流,人际关系又会发生怎样的变化?\n",
            "[02:58.000 --> 03:06.000] 语言不仅仅是一种工具,它既是连接人与人之间的桥梁,也是深入了解自己的钥匙。\n",
            "[03:06.000 --> 03:14.000] 一旦你的语言化能力得到提升,你会明显感觉到工作和生活中的沟通方式发生巨大变化。\n",
            "[03:14.000 --> 03:17.000] 这样的未来,其实触手可及。\n",
            "[03:17.000 --> 03:21.000] 语言化能力的提升,带来的三大变化。\n",
            "[03:21.000 --> 03:32.000] 作者指出,通过锻炼语言化能力,我们可以整理思绪,与他人沟通更加顺畅,同时培养更高质量的思考方式。\n",
            "[03:32.000 --> 03:38.000] 依传达能力的提升,最直接的变化,就是表达能力的显著提升。\n",
            "[03:38.000 --> 03:44.000] 当语言化能力提高后,你能更加精准地表达自己的想法和情感。\n",
            "[03:44.000 --> 03:51.000] 特别是,当你清楚自己该说什么时,就能更有自信的决定如何去说。\n",
            "[03:51.000 --> 03:56.000] 无论是工作还是日常生活,这都会带来巨大的好处。\n",
            "[03:56.000 --> 03:58.000] 二人际沟通更加顺畅。\n",
            "[03:58.000 --> 04:06.000] 不仅是在工作中,在家庭和社交关系中,语言化能力的提升也能带来巨大的改善。\n",
            "[04:06.000 --> 04:17.000] 比如,在夫妻或情侣之间,当出现分歧时,如果能够冷静、清晰地表达自己的感受,就能有效避免冲突。\n",
            "[04:17.000 --> 04:22.000] 以往你可能只会生气地说,总是我在做家务,我已经受够了。\n",
            "[04:22.000 --> 04:30.000] 但当你掌握了语言化技巧后,你可以更有条理地表达,最近我做家务的时间比以前多了。\n",
            "[04:30.000 --> 04:32.000] 我有点累了。\n",
            "[04:32.000 --> 04:38.000] 我知道你也很忙,但如果你能帮我分担一些,我会很开心。\n",
            "[04:38.000 --> 04:45.000] 这样一来,对方能够准确理解你的感受和需求,也更容易做出积极回应。\n",
            "[04:45.000 --> 04:49.000] 从而减少争吵,三自我表达更加自信。\n",
            "[04:49.000 --> 04:55.000] 当你能更精准地表达自己的想法,你在社交场合的表现也会大幅提升。\n",
            "[04:55.000 --> 05:07.000] 语言化能力,决定你的人生质量,也能让你在生活中减少误解,建立更深层次的连接,让任何人都能流畅表达的神奇语言化笔记法。\n",
            "[05:07.000 --> 05:12.000] 我们将讨论为什么倾听能力对提升语言化能力至关重要。\n",
            "[05:12.000 --> 05:18.000] 当提到语言化能力时,我们通常会联想到说的能力或写的能力。\n",
            "[05:18.000 --> 05:23.000] 然而作者指出,真正关键的其实是倾听的能力。\n",
            "[05:23.000 --> 05:29.000] 这里的倾听能力不仅只听取他人的话语,更包括倾听自己内心的声音。\n",
            "[05:29.000 --> 05:42.000] 例如,在表达某个观点之前,我们需要先自问自己真正想要表达的核心是什么,通过不断的自我提问,并尝试用语言清晰地表达出来。\n",
            "[05:42.000 --> 05:46.000] 这个过程对于锻炼语言化能力至关重要。\n",
            "[05:46.000 --> 05:55.000] 作者通过刻意训练这种倾听能力,深刻体会到自己的表达变得更加清晰,更容易让他人理解。\n",
            "[05:55.000 --> 06:02.000] 而倾听自己的内心,说白了,就是认真挖掘自己的真实想法和情感。\n",
            "[06:02.000 --> 06:08.000] 例如,我们可以经常问自己,为什么在这个场景下我会有这样的感受?\n",
            "[06:08.000 --> 06:11.000] 我真正想表达的核心是什么?\n",
            "[06:11.000 --> 06:19.000] 每天不断地向自己抛出这些问题,并努力找到答案,就能逐步培养自己的语言表达能力。\n",
            "[06:19.000 --> 06:26.000] 作者在长期实践这一方法的过程中,意识到倾听能力才是决定语言化能力高低的关键。\n",
            "[06:26.000 --> 06:34.000] 事实上,作者能够得出这一结论,与他长期担任广告文案策划的经验密切相关。\n",
            "[06:34.000 --> 06:43.000] 他在写广告文案时,90%的时间都在倾听,而真正用于写作的时间只占很小一部分。\n",
            "[06:43.000 --> 06:50.000] 这种先倾听,再表达的模式,使他的语言表达更加精准且有力。\n",
            "[06:50.000 --> 06:55.000] 乍一看,广告文案撰写似乎是一项创造语言的工作。\n",
            "[06:55.000 --> 07:00.000] 但实际上,他是从倾听开始的,想要精准的表达。\n",
            "[07:00.000 --> 07:07.000] 首先要做到的就是认真倾听客户和目标群体的声音,撰写优秀的文案。\n",
            "[07:07.000 --> 07:14.000] 必须深入了解客户对产品或服务的想法,以及目标用户的需求和痛点。\n",
            "[07:14.000 --> 07:23.000] 这通常包括对客户愿景的深入访谈,也可能是对市场数据的分析,从而推测出目标用户的画像。\n",
            "[07:23.000 --> 07:29.000] 然而,倾听并不仅限于他人,同样重要的是倾听自己的声音。\n",
            "[07:29.000 --> 07:35.000] 例如,我们需要思考自己为什么会被某个产品或主题吸引。\n",
            "[07:35.000 --> 07:45.000] 通过认真倾听自己的内心,我们才能更深刻的理解事物,并与他人产生共鸣,从而创造出能够触动人心的表达。\n",
            "[07:45.000 --> 07:52.000] 当我们真正磨练倾听能力后,头脑中模糊的思绪会变得异常清晰。\n",
            "[07:52.000 --> 08:01.000] 不仅在会议或演讲时能更顺畅地表达自己的观点,在撰写企划书或提案时的效率也会显著提高。\n",
            "[08:01.000 --> 08:07.000] 同时,在日常生活中,我们也能更准确地用语言表达自己的感受。\n",
            "[08:07.000 --> 08:15.000] 这些能力无论是在工作、人际关系还是个人生活中,都将带来难以估量的好处。\n",
            "[08:15.000 --> 08:21.000] 相信大家也能大致想象到,这种机能的提升将如何改变自己的生活。\n",
            "[08:21.000 --> 08:27.000] 不仅要倾听他人的话语,也不能忽视倾听自己内心的声音。\n",
            "[08:27.000 --> 08:31.000] 那么具体来说,我们该如何倾听自己呢?\n",
            "[08:31.000 --> 08:34.000] 接下来我将介绍一些关键的方法。\n",
            "[08:34.000 --> 08:39.000] 或许有些人会认为,倾听自己是一件很简单的事情。\n",
            "[08:39.000 --> 08:43.000] 毕竟我们不是在聆听别人,而是面对自己。\n",
            "[08:43.000 --> 08:49.000] 有人可能会觉得,我心里在想什么,难道还需要特别去听吗?\n",
            "[08:49.000 --> 08:51.000] 然而,事实并非如此。\n",
            "[08:51.000 --> 08:59.000] 我们很少真正倾听自己的内心,因为我们脑海中的大部分思考和情感并没有形成明确的语言表达。\n",
            "[08:59.000 --> 09:03.000] 比如,有些人会感觉,上班总是提不起进。\n",
            "[09:03.000 --> 09:08.000] 但又说不清具体的原因,工作缺乏成就感。\n",
            "[09:08.000 --> 09:11.000] 但不知道究竟哪里出了问题。\n",
            "[09:11.000 --> 09:15.000] 近期与伴侣朋友的关系似乎有些微妙的变化。\n",
            "[09:15.000 --> 09:20.000] 但不清楚问题出在哪里,对未来感到不安。\n",
            "[09:20.000 --> 09:23.000] 却无法准确描述自己的焦虑来源。\n",
            "[09:23.000 --> 09:26.000] 许多人都有类似的模糊感受。\n",
            "[09:26.000 --> 09:30.000] 但能把这种模糊清晰表达出来的人却寥寥无几。\n",
            "[09:30.000 --> 09:34.000] 我们往往比自己想象的更不了解自己。\n",
            "[09:34.000 --> 09:41.000] 请先接受这样一个事实,我们的内心并不会主动清晰地告诉我们真正的想法和感受。\n",
            "[09:41.000 --> 09:44.000] 甚至连我们自己都未必完全明白。\n",
            "[09:44.000 --> 09:50.000] 正因如此,想要真正理解自己,就必须主动去倾听。\n",
            "[09:50.000 --> 09:52.000] 而不是被动等待答案的出现。\n",
            "[09:52.000 --> 09:55.000] 那么,如何才能让自己愿意开口?\n",
            "[09:55.000 --> 09:58.000] 将内心的思绪转换为具体的语言呢?\n",
            "[09:58.000 --> 10:03.000] 例如,我似乎对突如其来的变化很难适应。\n",
            "[10:03.000 --> 10:05.000] 我不太擅长向他人求助。\n",
            "[10:05.000 --> 10:07.000] 容易把责任一个人承担。\n",
            "[10:07.000 --> 10:12.000] 记录小小的成功经验能有效增强我的自信。\n",
            "[10:12.000 --> 10:17.000] 通过这样的回顾,个人的情绪和行为模式就会变得清晰。\n",
            "[10:17.000 --> 10:21.000] 从而能够找到具体的改进方向,你觉得这些方法如何?\n",
            "[10:21.000 --> 10:27.000] 当你实践这些技巧时,倾听自己内心的习惯就会自然而然地养成。\n",
            "[10:27.000 --> 10:31.000] 而这种能力,正是提升语言表达力的基础。\n",
            "[10:31.000 --> 10:37.000] 可以帮助你更好地整理思绪,并精准地传达自己的想法和感受。\n",
            "[10:37.000 --> 10:41.000] 想要更好地理解自己,做出更明智的选择。\n",
            "[10:41.000 --> 10:46.000] 不妨先从倾听自己内心的声音开始,这一小小的改变。\n",
            "[10:46.000 --> 10:50.000] 或许能极大的提升你的沟通能力和生活质量。\n",
            "[10:50.000 --> 10:52.000] 接下来是第二部分。\n",
            "[10:52.000 --> 10:56.000] 每天只需三分钟,坚持五天即可见效。\n",
            "[10:56.000 --> 11:00.000] 任何人都能流畅表达的神奇语言化笔记术。\n",
            "[11:00.000 --> 11:05.000] 例如当别人突然问你,你怎么看时,你能够自信地回答。\n",
            "[11:05.000 --> 11:10.000] 面对决策时,你也能迅速整理思路并作出判断。\n",
            "[11:10.000 --> 11:17.000] 那么接下来我们就来介绍这套能让思维瞬间清晰的超高效方法,神奇的语言化笔记术。\n",
            "[11:18.000 --> 11:20.000] 它包含三个简单的步骤。\n",
            "[11:20.000 --> 11:22.000] 第一步,积累。\n",
            "[11:22.000 --> 11:24.000] 第二步,自问。\n",
            "[11:24.000 --> 11:26.000] 第三步,总结。\n",
            "[11:26.000 --> 11:29.000] 下面我们就分别解读每个步骤的具体内容。\n",
            "[11:29.000 --> 11:31.000] 第一步,积累。\n",
            "[11:31.000 --> 11:35.000] 第一步是记录日常发生的事件及相应的情绪。\n",
            "[11:35.000 --> 11:37.000] 这是语言化思维的基础。\n",
            "[11:37.000 --> 11:40.000] 也是在后续步骤中深入思考的关键。\n",
            "[11:40.000 --> 11:44.000] 关键点,记录让自己产生情绪波动的事件。\n",
            "[11:44.000 --> 11:46.000] 要整理思绪。\n",
            "[11:46.000 --> 11:50.000] 首先需要让它们以可视化的方式呈现出来。\n",
            "[11:50.000 --> 11:57.000] 我们的大脑中,事件和情绪往往交织在一起,模糊不清,导致思考混乱。\n",
            "[11:57.000 --> 12:05.000] 然而,通过具体的记录发生了什么和自己产生了什么感受,能够让这些信息变得清晰。\n",
            "[12:05.000 --> 12:08.000] 从而为下一步的深入探讨奠定基础。\n",
            "[12:08.000 --> 12:10.000] 如何记录?\n",
            "[12:10.000 --> 12:12.000] 具体描述事件。\n",
            "[12:12.000 --> 12:16.000] 采用谁做了什么发生了什么的方式进行记录。\n",
            "[12:16.000 --> 12:18.000] 尽量避免模糊的表达。\n",
            "[12:18.000 --> 12:20.000] 越详细越容易分析。\n",
            "[12:20.000 --> 12:22.000] 配套记录情绪。\n",
            "[12:22.000 --> 12:24.000] 写下当时的真实感受。\n",
            "[12:24.000 --> 12:27.000] 不要评判情绪的对错或合理性。\n",
            "[12:27.000 --> 12:29.000] 只需如实记录即可。\n",
            "[12:29.000 --> 12:31.000] 事件被上司表扬了。\n",
            "[12:31.000 --> 12:34.000] 情绪开心,感到安心。\n",
            "[12:34.000 --> 12:36.000] 是不是很简单?\n",
            "[12:36.000 --> 12:39.000] 记录小技巧,简短记录即可。\n",
            "[12:39.000 --> 12:41.000] 无需写成长篇大论。\n",
            "[12:41.000 --> 12:45.000] 甚至用简要的关键词或列表形式也可以。\n",
            "[12:45.000 --> 12:47.000] 不要错过记录的时机。\n",
            "[12:47.000 --> 12:52.000] 因为事件和情绪会随着时间的推移而变得模糊。\n",
            "[12:52.000 --> 12:54.000] 因此,最好在当天记录下来。\n",
            "[12:54.000 --> 12:56.000] 拓展情绪词汇。\n",
            "[12:56.000 --> 12:59.000] 比如,不只是开心或难过。\n",
            "[12:59.000 --> 13:04.000] 还可以更具体的写成成就感、焦虑、失落等。\n",
            "[13:04.000 --> 13:07.000] 以更精准的理解自己的内心状态。\n",
            "[13:07.000 --> 13:09.000] 通过这一过程。\n",
            "[13:09.000 --> 13:13.000] 你将逐渐掌握识别和整理自己思维与情绪的技巧。\n",
            "[13:13.000 --> 13:16.000] 从而迈向高效表达的第一步。\n",
            "[13:16.000 --> 13:21.000] 那么接下来我们来解析魔法语言化笔记法的不二。\n",
            "[13:21.000 --> 13:22.000] 请听。\n",
            "[13:22.000 --> 13:23.000] 在这个步骤中。\n",
            "[13:23.000 --> 13:25.000] 我们要向自己提出问题。\n",
            "[13:25.000 --> 13:29.000] 深入探究已经记录下来的事件和情感。\n",
            "[13:29.000 --> 13:33.000] 这是整理思维、洞察本质的最关键环节。\n",
            "[13:33.000 --> 13:36.000] 目的是弄清楚为什么我们会有这样的感受。\n",
            "[13:36.000 --> 13:38.000] 为什么要倾听。\n",
            "[13:38.000 --> 13:41.000] 我们常常停留在情绪的表面。\n",
            "[13:41.000 --> 13:45.000] 而忽略了隐藏在背后的真正原因和背景。\n",
            "[13:45.000 --> 13:48.000] 通过自问为什么会有这种感觉。\n",
            "[13:48.000 --> 13:51.000] 我们可以更深入地挖掘情绪的来源。\n",
            "[13:51.000 --> 13:53.000] 甚至探索自己的价值观。\n",
            "[13:53.000 --> 13:55.000] 如何进行倾听。\n",
            "[13:55.000 --> 13:57.000] 方法非常简单。\n",
            "[13:57.000 --> 13:59.000] 在第一步记录的笔记最后。\n",
            "[13:59.000 --> 14:01.000] 加上原因是什么。\n",
            "[14:01.000 --> 14:02.000] 这五个字。\n",
            "[14:02.000 --> 14:05.000] 就能轻松生成新的问题。\n",
            "[14:05.000 --> 14:07.000] 深入分析自己的感受。\n",
            "[14:07.000 --> 14:09.000] 如何实际操作。\n",
            "[14:09.000 --> 14:11.000] 设定三分钟目标。\n",
            "[14:11.000 --> 14:13.000] 取出笔记本和笔。\n",
            "[14:13.000 --> 14:16.000] 在左上角写下刚刚生成的问题。\n",
            "[14:16.000 --> 14:18.000] 利用线圈注他。\n",
            "[14:18.000 --> 14:21.000] 三分钟内写出至少五个答案。\n",
            "[14:21.000 --> 14:22.000] 在问题下方。\n",
            "[14:22.000 --> 14:25.000] 写下脑海中浮现的所有想法。\n",
            "[14:25.000 --> 14:27.000] 不用讲究文字工整。\n",
            "[14:27.000 --> 14:29.000] 也不用刻意组织语言。\n",
            "[14:29.000 --> 14:31.000] 想到什么就写什么。\n",
            "[14:31.000 --> 14:32.000] 利用倒计时。\n",
            "[14:32.000 --> 14:34.000] 提升专注力。\n",
            "[14:34.000 --> 14:36.000] 用手机的秒表功能。\n",
            "[14:36.000 --> 14:37.000] 计时三分钟。\n",
            "[14:37.000 --> 14:40.000] 虐待紧迫感的进行思考和记录。\n",
            "[14:40.000 --> 14:42.000] 会让思维更加集中。\n",
            "[14:42.000 --> 14:45.000] 魔法语言化笔记法的最终效果。\n",
            "[14:45.000 --> 14:47.000] 只要每天坚持这三个步骤。\n",
            "[14:47.000 --> 14:48.000] 记录。\n",
            "[14:48.000 --> 14:49.000] 倾听。\n",
            "[14:49.000 --> 14:50.000] 总结。\n",
            "[14:50.000 --> 14:51.000] 你就能在各种场合下。\n",
            "[14:51.000 --> 14:53.000] 提高思考速度。\n",
            "[14:53.000 --> 14:54.000] 迅速整理思绪。\n",
            "[14:54.000 --> 14:55.000] 并准确表达。\n",
            "[14:55.000 --> 14:57.000] 学会情绪管理。\n",
            "[14:57.000 --> 14:59.000] 减少焦虑和困惑。\n",
            "[14:59.000 --> 15:01.000] 做出冷静判断。\n",
            "[15:01.000 --> 15:03.000] 加深自我理解。\n",
            "[15:03.000 --> 15:04.000] 明确价值观。\n",
            "[15:04.000 --> 15:06.000] 行动更加有条理。\n",
            "[15:06.000 --> 15:08.000] 这个方法简单易行。\n",
            "[15:08.000 --> 15:10.000] 任何人都能实践。\n",
            "[15:10.000 --> 15:12.000] 每天的小小记录和深挖。\n",
            "[15:12.000 --> 15:15.000] 将极大的提升你的语言化能力。\n",
            "[15:15.000 --> 15:17.000] 并帮助你在生活的各个方面。\n",
            "[15:17.000 --> 15:19.000] 更清晰的表达自己。\n",
            "[15:19.000 --> 15:21.000] 希望你能把这三个步骤。\n",
            "[15:21.000 --> 15:23.000] 融入日常习惯。\n",
            "[15:23.000 --> 15:25.000] 整理自己的思绪。\n",
            "[15:25.000 --> 15:27.000] 拥有更充实的生活。\n",
            "[15:27.000 --> 15:29.000] 五天掌握言语化笔记法。\n",
            "[15:29.000 --> 15:30.000] 实际案例解析。\n",
            "[15:30.000 --> 15:32.000] 到目前为止。\n",
            "[15:32.000 --> 15:33.000] 我们已经讲解了。\n",
            "[15:33.000 --> 15:36.000] 如何在五天内掌握言语化笔记法。\n",
            "[15:36.000 --> 15:38.000] 尽管如此。\n",
            "[15:38.000 --> 15:39.000] 可能仍有人会担心。\n",
            "[15:39.000 --> 15:41.000] 虽然理解了方法。\n",
            "[15:41.000 --> 15:43.000] 但自己一个人实践时。\n",
            "[15:43.000 --> 15:44.000] 真的能做到吗?\n",
            "[15:44.000 --> 15:46.000] 因此我们来看看。\n",
            "[15:46.000 --> 15:48.000] 一位三十多岁的上班族的。\n",
            "[15:48.000 --> 15:49.000] 具体案例。\n",
            "[15:49.000 --> 15:50.000] 借此进一步确认。\n",
            "[15:50.000 --> 15:52.000] 实际操作的流程。\n",
            "[15:52.000 --> 15:53.000] 案例一。\n",
            "[15:53.000 --> 15:55.000] 因妻子的一句话生气的。\n",
            "[15:55.000 --> 15:56.000] 一先生。\n",
            "[15:56.000 --> 15:57.000] 有一天早晨。\n",
            "[15:57.000 --> 15:59.000] A先生出门上班时。\n",
            "[15:59.000 --> 16:01.000] 忘记了倒垃圾。\n",
            "[16:01.000 --> 16:02.000] 晚上回家后。\n",
            "[16:02.000 --> 16:03.000] 他的妻子。\n",
            "[16:03.000 --> 16:04.000] 带着不满的语气。\n",
            "[16:04.000 --> 16:05.000] 对他说。\n",
            "[16:05.000 --> 16:07.000] 你今天没倒垃圾吧。\n",
            "[16:07.000 --> 16:09.000] 听到这句话。\n",
            "[16:09.000 --> 16:11.000] A先生顿时感到不快。\n",
            "[16:11.000 --> 16:12.000] 脱口而出。\n",
            "[16:12.000 --> 16:13.000] 你发现了。\n",
            "[16:13.000 --> 16:14.000] 为什么不自己倒。\n",
            "[16:14.000 --> 16:16.000] 说完房间里的气氛。\n",
            "[16:16.000 --> 16:18.000] 瞬间变得沉重起来。\n",
            "[16:18.000 --> 16:20.000] 像这样的日常小事。\n",
            "[16:20.000 --> 16:22.000] 如果仅凭情绪反应。\n",
            "[16:22.000 --> 16:24.000] 只会导致关系恶化。\n",
            "[16:24.000 --> 16:25.000] 然而。\n",
            "[16:25.000 --> 16:27.000] 若能回顾这一场景。\n",
            "[16:27.000 --> 16:29.000] 深入剖析自己的内心。\n",
            "[16:29.000 --> 16:31.000] 便有机会获得新的发现。\n",
            "[16:31.000 --> 16:33.000] 甚至促进自身成长。\n",
            "[16:33.000 --> 16:34.000] 因此。\n",
            "[16:34.000 --> 16:35.000] 他决定。\n",
            "[16:35.000 --> 16:37.000] 尝试用言语化笔记法。\n",
            "[16:37.000 --> 16:39.000] 来整理自己的情绪。\n",
            "[16:39.000 --> 16:41.000] 魔法言语化笔记法三步骤。\n",
            "[16:41.000 --> 16:42.000] 记录。\n",
            "[16:42.000 --> 16:43.000] 储存。\n",
            "[16:43.000 --> 16:45.000] 写下事件及自己的感受。\n",
            "[16:45.000 --> 16:46.000] 倾听。\n",
            "[16:46.000 --> 16:47.000] 自问。\n",
            "[16:47.000 --> 16:48.000] 提出。\n",
            "[16:48.000 --> 16:49.000] 为什么。\n",
            "[16:49.000 --> 16:50.000] 并深入思考。\n",
            "[16:50.000 --> 16:51.000] 总结。\n",
            "[16:51.000 --> 16:52.000] 归纳。\n",
            "[16:52.000 --> 16:53.000] 整理思考内容。\n",
            "[16:53.000 --> 16:54.000] 得出结论。\n",
            "[16:54.000 --> 16:55.000] 首先执行第一步。\n",
            "[16:55.000 --> 16:56.000] 记录。\n",
            "[16:56.000 --> 16:57.000] 简单写下。\n",
            "[16:57.000 --> 16:58.000] 忘记倒垃圾。\n",
            "[16:58.000 --> 16:59.000] 妻子对我抱怨。\n",
            "[16:59.000 --> 17:01.000] 我因此感到愤怒。\n",
            "[17:01.000 --> 17:03.000] 接着他进入第二步。\n",
            "[17:03.000 --> 17:04.000] 倾听。\n",
            "[17:04.000 --> 17:05.000] 向自己提问。\n",
            "[17:05.000 --> 17:07.000] 为什么妻子因垃圾问题抱怨。\n",
            "[17:07.000 --> 17:09.000] 我就会生气。\n",
            "[17:09.000 --> 17:10.000] 随后。\n",
            "[17:10.000 --> 17:12.000] 他设定三分钟计时器。\n",
            "[17:12.000 --> 17:13.000] 在此期间。\n",
            "[17:13.000 --> 17:15.000] 尽可能写下所有想到的原因。\n",
            "[17:15.000 --> 17:16.000] 最初。\n",
            "[17:16.000 --> 17:17.000] 他的想法。\n",
            "[17:17.000 --> 17:18.000] 主要是。\n",
            "[17:18.000 --> 17:20.000] 为什么总是我负责倒垃圾。\n",
            "[17:20.000 --> 17:22.000] 希望他能多感激我等。\n",
            "[17:22.000 --> 17:24.000] 倾向于反驳对方。\n",
            "[17:24.000 --> 17:26.000] 但随着书写的进行。\n",
            "[17:26.000 --> 17:28.000] 他逐渐转换了视角。\n",
            "[17:28.000 --> 17:29.000] 开始意识到。\n",
            "[17:29.000 --> 17:31.000] 也许是因为早上太忙。\n",
            "[17:31.000 --> 17:33.000] 倒垃圾让我感到压力。\n",
            "[17:33.000 --> 17:35.000] 或许我自己忘记倒垃圾。\n",
            "[17:35.000 --> 17:37.000] 也有点内疚。\n",
            "[17:37.000 --> 17:39.000] 这种深入反思的过程。\n",
            "[17:39.000 --> 17:41.000] 在心理学上被称为内省。\n",
            "[17:41.000 --> 17:44.000] 内省是一种回顾自身言行。\n",
            "[17:44.000 --> 17:46.000] 与情绪的行为。\n",
            "[17:46.000 --> 17:47.000] 能帮助我们。\n",
            "[17:47.000 --> 17:48.000] 发现自己的步途。\n",
            "[17:48.000 --> 17:50.000] 及固有偏见。\n",
            "[17:50.000 --> 17:52.000] 并进而接受他们。\n",
            "[17:52.000 --> 17:54.000] 迈出新的步伐。\n",
            "[17:54.000 --> 17:55.000] 此外。\n",
            "[17:55.000 --> 17:56.000] 书写本身。\n",
            "[17:56.000 --> 17:57.000] 也能帮助我们。\n",
            "[17:57.000 --> 17:59.000] 客观化情绪。\n",
            "[17:59.000 --> 18:01.000] 使其更加清晰。\n",
            "[18:01.000 --> 18:03.000] 改善人际关系的关键。\n",
            "[18:03.000 --> 18:05.000] 在亲密关系中。\n",
            "[18:05.000 --> 18:06.000] 我们往往容易。\n",
            "[18:06.000 --> 18:08.000] 对对方期待过高。\n",
            "[18:08.000 --> 18:10.000] 当期待落空时。\n",
            "[18:10.000 --> 18:12.000] 就会感到沮丧或愤怒。\n",
            "[18:12.000 --> 18:13.000] 因此。\n",
            "[18:13.000 --> 18:15.000] 停下来审视自己的情绪。\n",
            "[18:15.000 --> 18:17.000] 是维护良好关系的。\n",
            "[18:17.000 --> 18:18.000] 重要一步。\n",
            "[18:18.000 --> 18:20.000] 如果你下次遇到类似情况。\n",
            "[18:20.000 --> 18:22.000] 不妨尝试先写下来。\n",
            "[18:22.000 --> 18:24.000] 而不是直接与对方争论。\n",
            "[18:24.000 --> 18:25.000] 这样。\n",
            "[18:25.000 --> 18:27.000] 你不仅能更好的管理情绪。\n",
            "[18:27.000 --> 18:29.000] 也能改善人际关系。\n",
            "[18:29.000 --> 18:31.000] 甚至促进自身成长。\n",
            "[18:31.000 --> 18:33.000] 言语化的力量。\n",
            "[18:33.000 --> 18:35.000] 通过提问与书写。\n",
            "[18:35.000 --> 18:36.000] 模糊的情绪。\n",
            "[18:36.000 --> 18:38.000] 会变得具体。\n",
            "[18:38.000 --> 18:39.000] 甚至能帮助我们。\n",
            "[18:39.000 --> 18:41.000] 找到幸福的本质。\n",
            "[18:41.000 --> 18:43.000] 创造更多轻松的交流时间。\n",
            "[18:43.000 --> 18:46.000] 或更加珍惜日常的小幸福。\n",
            "[18:46.000 --> 18:48.000] 这正是言语化笔记法的。\n",
            "[18:48.000 --> 18:49.000] 价值所在。\n",
            "[18:49.000 --> 18:51.000] 让我们从最初的。\n",
            "[18:51.000 --> 18:53.000] 让我们从日常的情绪中。\n",
            "[18:53.000 --> 18:55.000] 找到真正重要的事物。\n",
            "[18:55.000 --> 18:57.000] 并付诸实践。\n",
            "[18:57.000 --> 18:59.000] 使生活更加充实。\n",
            "[18:59.000 --> 19:00.000] 如果你最近。\n",
            "[19:00.000 --> 19:02.000] 也有让你烦躁或开心的事情。\n",
            "[19:02.000 --> 19:04.000] 不妨试试看。\n",
            "[19:04.000 --> 19:06.000] 记录写下事件及你的感受。\n",
            "[19:06.000 --> 19:08.000] 倾听提出为什么。\n",
            "[19:08.000 --> 19:10.000] 并展开思考。\n",
            "[19:10.000 --> 19:12.000] 整理思考内容。\n",
            "[19:12.000 --> 19:14.000] 得出自己的结论。\n",
            "[19:14.000 --> 19:16.000] 通过这个简单的方法。\n",
            "[19:16.000 --> 19:18.000] 你不仅能更好的管理情绪。\n",
            "[19:18.000 --> 19:20.000] 还能更深入的理解自己。\n",
            "[19:20.000 --> 19:22.000] 从而迈向更加充实的生活。\n",
            "[19:22.000 --> 19:24.000] 尝试用笔记的方式。\n",
            "[19:24.000 --> 19:26.000] 与自己对话。\n",
            "[19:26.000 --> 19:28.000] 让表达变得自然流畅。\n",
            "[19:28.000 --> 19:30.000] 让生活变得更加充实。\n",
            "[19:50.000 --> 19:52.000] 再次感谢大家的支持。\n",
            "[19:52.000 --> 19:54.000] 我们下期再见。\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'drive_whisper_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-da8a821d8b76>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_format\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.vtt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.srt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mexportTranscriptFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mexportTranscriptFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutput_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-da8a821d8b76>\u001b[0m in \u001b[0;36mexportTranscriptFile\u001b[0;34m(ext)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexportTranscriptFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mlocal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_path_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mvideo_path_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mexport_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive_whisper_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mvideo_path_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         shutil.copy(\n\u001b[1;32m    130\u001b[0m             \u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'drive_whisper_path' is not defined"
          ]
        }
      ],
      "source": [
        "#@markdown # **Run the model** 🚀\n",
        "\n",
        "#@markdown Run this cell to execute the transcription of the video. This can take a while and very based on the length of the video and the number of parameters of the model selected above.\n",
        "\n",
        "#@markdown ## **Parameters** ⚙️\n",
        "\n",
        "#@markdown ### **Behavior control**\n",
        "#@markdown ---\n",
        "language = \"Chinese\" #@param ['Auto detection', 'Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Assamese', 'Azerbaijani', 'Bashkir', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Breton', 'Bulgarian', 'Burmese', 'Castilian', 'Catalan', 'Chinese', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian', 'Faroese', 'Finnish', 'Flemish', 'French', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Lao', 'Latin', 'Latvian', 'Letzeburgesch', 'Lingala', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Moldavian', 'Moldovan', 'Mongolian', 'Myanmar', 'Nepali', 'Norwegian', 'Nynorsk', 'Occitan', 'Panjabi', 'Pashto', 'Persian', 'Polish', 'Portuguese', 'Punjabi', 'Pushto', 'Romanian', 'Russian', 'Sanskrit', 'Serbian', 'Shona', 'Sindhi', 'Sinhala', 'Sinhalese', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog', 'Tajik', 'Tamil', 'Tatar', 'Telugu', 'Thai', 'Tibetan', 'Turkish', 'Turkmen', 'Ukrainian', 'Urdu', 'Uzbek', 'Valencian', 'Vietnamese', 'Welsh', 'Yiddish', 'Yoruba']\n",
        "#@markdown > Language spoken in the audio, use `Auto detection` to let Whisper detect the language.\n",
        "#@markdown ---\n",
        "verbose = 'Live transcription' #@param ['Live transcription', 'Progress bar', 'None']\n",
        "#@markdown > Whether to print out the progress and debug messages.\n",
        "#@markdown ---\n",
        "output_format = 'all' #@param ['txt', 'vtt', 'srt', 'tsv', 'json', 'all']\n",
        "#@markdown > Type of file to generate to record the transcription.\n",
        "#@markdown ---\n",
        "task = 'transcribe' #@param ['transcribe', 'translate']\n",
        "#@markdown > Whether to perform X->X speech recognition (`transcribe`) or X->English translation (`translate`).\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown <br/>\n",
        "\n",
        "#@markdown ### **Optional: Fine tunning**\n",
        "#@markdown ---\n",
        "temperature = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to use for sampling.\n",
        "#@markdown ---\n",
        "temperature_increment_on_fallback = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to increase when falling back when the decoding fails to meet either of the thresholds below.\n",
        "#@markdown ---\n",
        "best_of = 5 #@param {type:\"integer\"}\n",
        "#@markdown > Number of candidates when sampling with non-zero temperature.\n",
        "#@markdown ---\n",
        "beam_size = 8 #@param {type:\"integer\"}\n",
        "#@markdown > Number of beams in beam search, only applicable when temperature is zero.\n",
        "#@markdown ---\n",
        "patience = 1.0 #@param {type:\"number\"}\n",
        "#@markdown > Optional patience value to use in beam decoding, as in [*Beam Decoding with Controlled Patience*](https://arxiv.org/abs/2204.05424), the default (1.0) is equivalent to conventional beam search.\n",
        "#@markdown ---\n",
        "length_penalty = -0.05 #@param {type:\"slider\", min:-0.05, max:1, step:0.05}\n",
        "#@markdown > Optional token length penalty coefficient (alpha) as in [*Google's Neural Machine Translation System*](https://arxiv.org/abs/1609.08144), set to negative value to uses simple length normalization.\n",
        "#@markdown ---\n",
        "suppress_tokens = \"-1\" #@param {type:\"string\"}\n",
        "#@markdown > Comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations.\n",
        "#@markdown ---\n",
        "initial_prompt = \"\" #@param {type:\"string\"}\n",
        "#@markdown > Optional text to provide as a prompt for the first window.\n",
        "#@markdown ---\n",
        "condition_on_previous_text = True #@param {type:\"boolean\"}\n",
        "#@markdown > if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop.\n",
        "#@markdown ---\n",
        "fp16 = True #@param {type:\"boolean\"}\n",
        "#@markdown > whether to perform inference in fp16.\n",
        "#@markdown ---\n",
        "compression_ratio_threshold = 2.4 #@param {type:\"number\"}\n",
        "#@markdown > If the gzip compression ratio is higher than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "logprob_threshold = -1.0 #@param {type:\"number\"}\n",
        "#@markdown > If the average log probability is lower than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "no_speech_threshold = 0.6 #@param {type:\"slider\", min:-0.0, max:1, step:0.05}\n",
        "#@markdown > If the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence.\n",
        "#@markdown ---\n",
        "\n",
        "verbose_lut = {\n",
        "    'Live transcription': True,\n",
        "    'Progress bar': False,\n",
        "    'None': None\n",
        "}\n",
        "\n",
        "args = dict(\n",
        "    language = (None if language == \"Auto detection\" else language),\n",
        "    verbose = verbose_lut[verbose],\n",
        "    task = task,\n",
        "    temperature = temperature,\n",
        "    temperature_increment_on_fallback = temperature_increment_on_fallback,\n",
        "    best_of = best_of,\n",
        "    beam_size = beam_size,\n",
        "    patience=patience,\n",
        "    length_penalty=(length_penalty if length_penalty>=0.0 else None),\n",
        "    suppress_tokens=suppress_tokens,\n",
        "    initial_prompt=(None if not initial_prompt else initial_prompt),\n",
        "    condition_on_previous_text=condition_on_previous_text,\n",
        "    fp16=fp16,\n",
        "    compression_ratio_threshold=compression_ratio_threshold,\n",
        "    logprob_threshold=logprob_threshold,\n",
        "    no_speech_threshold=no_speech_threshold\n",
        ")\n",
        "\n",
        "temperature = args.pop(\"temperature\")\n",
        "temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n",
        "if temperature_increment_on_fallback is not None:\n",
        "    temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n",
        "else:\n",
        "    temperature = [temperature]\n",
        "\n",
        "if Model.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n",
        "    warnings.warn(f\"{Model} is an English-only model but receipted '{args['language']}'; using English instead.\")\n",
        "    args[\"language\"] = \"en\"\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    display(Markdown(f\"### {video_path_local}\"))\n",
        "\n",
        "    video_transcription = whisper.transcribe(\n",
        "        whisper_model,\n",
        "        str(video_path_local),\n",
        "        temperature=temperature,\n",
        "        **args,\n",
        "    )\n",
        "\n",
        "    # Save output\n",
        "    whisper.utils.get_writer(\n",
        "        output_format=output_format,\n",
        "        output_dir=video_path_local.parent\n",
        "    )(\n",
        "        video_transcription,\n",
        "        str(video_path_local.stem),\n",
        "        options=dict(\n",
        "            highlight_words=False,\n",
        "            max_line_count=None,\n",
        "            max_line_width=None,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    def exportTranscriptFile(ext: str):\n",
        "        local_path = video_path_local.parent / video_path_local.with_suffix(ext).name\n",
        "        export_path = drive_whisper_path / video_path_local.with_suffix(ext).name\n",
        "        shutil.copy(\n",
        "            local_path,\n",
        "            export_path\n",
        "        )\n",
        "        display(Markdown(f\"**Transcript file created: {export_path}**\"))\n",
        "\n",
        "    if output_format==\"all\":\n",
        "        for ext in ('.txt', '.vtt', '.srt', '.tsv', '.json'):\n",
        "            exportTranscriptFile(ext)\n",
        "    else:\n",
        "        exportTranscriptFile(\".\" + output_format)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ad6n1m4deAHp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}